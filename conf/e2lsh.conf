# seed:2
outputPath=/ysong/output

# # <<<for SIFT1B - learn, text
# inputPath:/jinfeng/image/ANN_SIFT1B/bigann_learn.bvecs
# # inputPath:/jinfeng/image/ANN_SIFT1B/bigann_query.bvecs-1000
# queryPath:/jinfeng/image/ANN_SIFT1B/bigann_query.bvecs-1000
# bands:1
# # outputPath:/jinfeng/sift1b-learn/L-19
# rows:20
# dimension:128
# maxIteration:1
# W:600
# # >>>

# #<<<<< for ANN_SIFT1M, binary, cannot mix sift1b and gist1m, the formats are different
# queryPath=hdfs:///jinfeng/image/ANN_SIFT1M/sift_query.fvecs/part-r-00000
# itemPath=hdfs:///jinfeng/image/ANN_SIFT1M/sift_base.fvecs/part-r-00000
# # itemPath=hdfs:///jinfeng/image/ANN_SIFT1M/sift_base.fvecs
# band=1
# row=20
# W=600
# dimension=128
# maxIteration=1
# #>>>>
# # # >>>

#<<<<< for small sample (2 query, 7 item, each has 5 dimensions)

# If you have configured HDFS, you can first upload your files to HDFS and then load input files from HDFS, using the following params. 

# Load input files from HDFS:
#itemPath=hdfs:///ysong/data/item_ss.txt
#queryPath=hdfs:///ysong/data/query_ss.txt
#itemPath=hdfs:///ysong/data/item_ss.bin
#queryPath=hdfs:///ysong/data/query_ss.bin

# Load input files from local:
itemPath=nfs://../apps/e2lsh/data/item_ss.txt
queryPath=nfs://../apps/e2lsh/data/query_ss.txt

band=1
row=20
W=600
dimension=5
maxIteration=1
#>>>>

# the following is for cluster configuration
master_host=proj10
master_port=13919
comm_port=13242

hdfs_namenode=proj10
hdfs_namenode_port=9000

serve=1
hostname=proj10
port=2016

[worker]
info=proj10:2
#info=w1:20
#info=w2:20
#info=w3:20
#info=w4:20
#info=w5:20
#info=w6:20
#info=w7:20
#info=w8:20
#info=w9:20
#info=w10:20
#info=w11:20
#info=w12:20
#info=w13:20
#info=w14:20
#info=w15:20
#info=w16:20
#info=w17:20
#info=w18:20
#info=w19:20
#info=w20:20
